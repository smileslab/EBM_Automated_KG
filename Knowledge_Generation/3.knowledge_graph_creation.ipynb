{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install keras --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install biobert-embedding --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 13:02:33.578979: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 13:02:33.768859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-10-10 13:02:33.768897: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-10 13:02:33.806057: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 13:02:34.850314: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-10-10 13:02:34.850621: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-10-10 13:02:34.850644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "#from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#tqdm.pandas(desc=\"progress-bar\")\n",
    "#from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import gensim\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,make_scorer,precision_score,recall_score,roc_auc_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import xgboost as xgb\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "#from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "#import plotly.graph_objs as go\n",
    "#import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "#from plotly.offline import iplot\n",
    "#cufflinks.go_offline()\n",
    "#cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "from bs4 import BeautifulSoup\n",
    "#from rouge import Rouge\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import getpass\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation \n",
    "\n",
    "from biobert_embedding.embedding import BiobertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read input files for Covid 19-Metadata / Brain Aneurysm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/oss/conda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (5,13,14,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Covid 19 raw data\n",
    "df_covid19_raw=pd.read_csv('./data/metadata.csv')\n",
    "# Covid 19 PICO classified data\n",
    "##df_covid19_PICO=pd.read_csv('./data/df_covid19_PICO.csv')\n",
    "# Covid 19 PICO data with biobert encoder\n",
    "df_covid19_PICO_biobert=pd.read_csv('./data/df_covid19_PICO_biobert_encoded_combined.csv')\n",
    "\n",
    "df_covid19_PICO_biobert.dropna(subset=['sent'],inplace=True)\n",
    "df_covid19_PICO_biobert.dropna(subset=['sent_embedding'],inplace=True)\n",
    "df_covid19_PICO_biobert['sent_embedding_array']=df_covid19_PICO_biobert['sent_embedding'].apply(lambda x:np.array(eval(x.replace('\\n','').replace('tensor(','').replace(')',''))))\n",
    "\n",
    " # CB Data\n",
    "\n",
    "# CB data with bioBERT encoder\n",
    "df_BA_APIRO_biobert=pd.read_csv('./data/df_APIRO_Dataset_biobert_encoded_combined.csv')\n",
    "# remove nan and null\n",
    "df_BA_APIRO_biobert.dropna(subset=['sent'],inplace=True)\n",
    "df_BA_APIRO_biobert.dropna(subset=['sent_embedding'],inplace=True)\n",
    "df_BA_APIRO_biobert['sent_embedding_array']=df_BA_APIRO_biobert['sent_embedding'].apply(lambda x:np.array(eval(x.replace('\\n','').replace('tensor(','').replace(')',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BA_APIRO_biobert['aimoprc_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Cluster to indetify different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_clustering=df_covid19_raw[['pmcid','pubmed_id','abstract']]\n",
    "df_covid19_clustering.dropna(subset=['abstract'],inplace=True)\n",
    "df_covid19_clustering = df_covid19_clustering.reset_index(drop=True)\n",
    "df_covid19_clustering.rename(columns={'abstract':'sent'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_clustering.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    @author Fakhare Alam\n",
    "    '''\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_token_generator(df):\n",
    "    '''\n",
    "    @author : Fakhare Alam\n",
    "    '''\n",
    " \n",
    "    #The maximum number of words to be used\n",
    "    MAX_NB_WORDS = 50000\n",
    "    # Max number of words in each sentence\n",
    "    MAX_SEQUENCE_LENGTH = 250\n",
    "    # Embeddng dimension\n",
    "    EMBEDDING_DIM = 100\n",
    "    df['sent'] = df['sent'].apply(clean_text)\n",
    "    df['sent'] =df['sent'].str.replace('\\d+', '')\n",
    "    tokenizer_keras = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer_keras.fit_on_texts(df['sent'].values)\n",
    "    word_index_keras = tokenizer_keras.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index_keras))\n",
    "\n",
    "    X_keras = tokenizer_keras.texts_to_sequences(df['sent'].values)\n",
    "    X_keras = pad_sequences(X_keras, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of data tensor:',X_keras.shape)\n",
    "    return X_keras\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras data tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw Covid 19\n",
    "X_covid=keras_token_generator(df_covid19_clustering)\n",
    "# Covid 19 with PICO\n",
    "X_covid_PICO=keras_token_generator(df_covid19_PICO_biobert)#keras_token_generator(df_covid19_PICO)\n",
    "# CB\n",
    "X_CB=keras_token_generator(df_BA_APIRO_biobert)#keras_token_generator(df_BA_APIRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analsysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PCA(X):\n",
    "    '''\n",
    "    '''\n",
    "    pca_2=PCA(n_components=2)\n",
    "    # Scaled Data\n",
    "    pca_data=pca_2.fit_transform(X)\n",
    "\n",
    "    #Pickle the PCA model\n",
    "    #pickle.dump(pca_2_covid, open('pca', 'wb'))\n",
    "\n",
    "    df_pca_data=pd.DataFrame(data=pca_data,columns=['first','second'])\n",
    "    return df_pca_data,pca_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_pca_data,pca_data_covid=create_PCA(X_covid)\n",
    "df_covid_pca_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BA_APIRO=pd.read_csv('./data/APIRO_Dataset.csv').drop(columns='Unnamed: 0',axis=1)\n",
    "df_CB_pca_data,pca_data_CB=create_PCA(keras_token_generator(df_BA_APIRO))\n",
    "df_CB_pca_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA performence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Values and Eigen vector\n",
    "print('Eigen Values',pca_2_CB.explained_variance_)\n",
    "#print('Eigen Vectors',pca_2.components_)\n",
    "print('\\n')\n",
    "print('Explained Variance Ratio',pca_2_CB.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(pca_data_CB[:,0],pca_data_CB[:,1])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('New projection using PCA with 2 dimensions',fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means clustering to optimize number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_iteration(pca_data):\n",
    "    \n",
    "    '''\n",
    "    @author - Fakhare Alam\n",
    "    '''\n",
    "    inertias = [] \n",
    "    mapping_inertia = {} \n",
    "    K = range(2,10) \n",
    "    mapping_inertia['#clusetrs']='SSE'\n",
    "    for k in K: \n",
    "        #Building and fitting the model \n",
    "        kmeanModel = KMeans(n_clusters=k) \n",
    "        kmeanModel.fit(pca_data)        \n",
    "        inertias.append(kmeanModel.inertia_) \n",
    "        mapping_inertia[k] = kmeanModel.inertia_ \n",
    "\n",
    "    for key,val in mapping_inertia.items(): \n",
    "        print(str(key)+' : '+str(val))\n",
    "    plt.plot(K, inertias, 'bx-') \n",
    "    plt.xlabel('k (number of clusters)') \n",
    "    plt.ylabel('Sum of Squared Distances') \n",
    "    plt.title('Covid 19 dataset clustering') \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Covid 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_iteration(pca_data_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_iteration(pca_data_CB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map on optimized number of clustering and map it back to original sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_Kmeans(X, nclust=2):\n",
    "    '''\n",
    "    @ Author - Fakhare Alam\n",
    "    This method will gemerate cluster based on\n",
    "    givennumber of clusters\n",
    "    '''\n",
    "    model = KMeans(n_clusters=nclust)\n",
    "    model.fit(X)\n",
    "    clust_labels = model.predict(X)\n",
    "    cent = model.cluster_centers_\n",
    "    \n",
    "    # Pickle the Kmeans Model\n",
    "    #pickle.dump(model, open('Kmeans', 'wb'))\n",
    "    return (clust_labels, cent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Covid 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Cluster\n",
    "knn_clust_labels, cent = do_Kmeans(pca_data_covid, 4)\n",
    "\n",
    "#Combine label with scaled (PCA) data and raw data\n",
    "df_covid19_clustering[['First PCA Component','Second PCA Component']]=df_covid_pca_data[['first','second']]\n",
    "df_covid19_clustering['knn_clust_labels']=knn_clust_labels\n",
    "\n",
    "# Check Value Counts of all the label\n",
    "df_covid19_clustering['knn_clust_labels'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=\"First PCA Component\", y=\"Second PCA Component\", hue=\"knn_clust_labels\",data=df_covid19_clustering,palette=\"Set2\",legend='full').set(title='Covid 19 cluster visulization using PCA components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Cluster\n",
    "knn_clust_labels, cent = do_Kmeans(pca_data_CB, 4)\n",
    "print('Cluster Labels',knn_clust_labels)\n",
    "print('Centroid of cluster',cent)\n",
    "\n",
    "#Combine label with scaled (PCA) data and raw data\n",
    "df_BA_APIRO[['First PCA Component','Second PCA Component']]=df_CB_pca_data[['first','second']]\n",
    "df_BA_APIRO['knn_clust_labels']=knn_clust_labels\n",
    "\n",
    "# Check Value Counts of all the label\n",
    "df_BA_APIRO['knn_clust_labels'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=\"First PCA Component\", y=\"Second PCA Component\", hue=\"knn_clust_labels\",data=df_BA_APIRO,palette=\"Set2\",legend='full').set(title='Cerebral Aneurysm data cluster visulization using PCA omponents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling - to determine theme in identifies cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic multi class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_classifier(X_train,Y_train,X_test,Y_test,model,classifier):\n",
    "    '''\n",
    "    @author : Fakhare Alam\n",
    "    '''\n",
    "    print(classifier)\n",
    "    print('\\n')\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred=model.predict(X_test)\n",
    "    print('Confusion Matrix\\n')\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print('Classification Report\\n')\n",
    "    print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB Dataset -Keras embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_CB = df_BA_APIRO_biobert['aimoprc_category']\n",
    "Y_L_CB = pd.get_dummies(df_BA_APIRO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_L_CB.shape)\n",
    "X_CB_train, X_CB_test, Y_CB_train, Y_CB_test = train_test_split(X_CB,Y_CB, test_size = 0.10, random_state = 42)\n",
    "X_CB_train_LSTM, X_CB_test_LSTM,Y_CB_train_LSTM, Y_CB_test_LSTM = train_test_split(X_CB,Y_L_CB, test_size = 0.10, random_state = 42)\n",
    "print('train dataset shape\\n')\n",
    "print(X_CB_train.shape,Y_CB_train.shape)\n",
    "print('test dataset shape\\n')\n",
    "print(X_CB_test.shape,Y_CB_test.shape)\n",
    "\n",
    "# Base Model \n",
    "logreg_CB=LogisticRegression()\n",
    "knn_CB = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "adaboost_CB = AdaBoostClassifier(random_state=1)\n",
    "Gboost_CB = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "mlp_CB = MLPClassifier(hidden_layer_sizes=(100, 70, 30,20,10), max_iter=1000)\n",
    "\n",
    "run_models=[logreg_CB,knn_CB,adaboost_CB,Gboost_CB,mlp_CB]\n",
    "\n",
    "for model in run_models:\n",
    "    multiclass_classifier(X_CB_train,Y_CB_train,X_CB_test,Y_CB_test,model,str(model))\n",
    "\n",
    "# LSTM Model\n",
    "#The maximum number of words to be used\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each sentence\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# Embeddng dimension\n",
    "EMBEDDING_DIM = 100\n",
    "lstm_model_CB = Sequential()\n",
    "lstm_model_CB.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_CB.shape[1]))\n",
    "lstm_model_CB.add(SpatialDropout1D(0.2))\n",
    "lstm_model_CB.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_CB.add(Dense(5, activation='softmax'))\n",
    "lstm_model_CB.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm_model_CB.summary())\n",
    "\n",
    "###\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = lstm_model_CB.fit(X_CB_train_LSTM, Y_CB_train_LSTM, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM Keras Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM Keras Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "Y_CB_pred = np.argmax(lstm_model_CB.predict(X_CB_test_LSTM),axis=1)\n",
    "print(confusion_matrix(np.argmax(Y_CB_test_LSTM,axis=1),Y_CB_pred))\n",
    "print(classification_report(np.argmax(Y_CB_test_LSTM,axis=1),Y_CB_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB Dataset-bioBERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CB_biobert=np.vstack(df_BA_APIRO_biobert['sent_embedding_array'])\n",
    "X_CB_biobert=X_CB_biobert[:, :50]\n",
    "Y_CB_biobert = df_BA_APIRO_biobert['aimoprc_category']\n",
    "Y_L_CB_biobert = pd.get_dummies(df_BA_APIRO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_L_CB_biobert.shape)\n",
    "X_CB_train_biobert, X_CB_test_biobert, Y_CB_train_biobert, Y_CB_test_biobert = train_test_split(X_CB_biobert,Y_CB_biobert, test_size = 0.10, random_state = 42)\n",
    "X_CB_train_LSTM_biobert, X_CB_test_LSTM_biobert,Y_CB_train_LSTM_biobert, Y_CB_test_LSTM_biobert = train_test_split(X_CB_biobert,Y_L_CB_biobert, test_size = 0.10, random_state = 42)\n",
    "print('train dataset shape')\n",
    "print(X_CB_train_biobert.shape,Y_CB_train_biobert.shape)\n",
    "print('test dataset shape')\n",
    "print(X_CB_test_biobert.shape,Y_CB_test_biobert.shape)\n",
    "\n",
    "# Base Model\n",
    "logreg_CB_biobert=LogisticRegression()\n",
    "knn_CB_biobert = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "adaboost_CB_biobert = AdaBoostClassifier(random_state=1)\n",
    "Gboost_CB_biobert = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "mlp_CB_biobert = MLPClassifier(hidden_layer_sizes=(100, 70, 30,20,10), max_iter=1000)\n",
    "\n",
    "run_models=[logreg_CB_biobert,knn_CB_biobert,adaboost_CB_biobert,Gboost_CB_biobert,mlp_CB_biobert]\n",
    "\n",
    "for model in run_models:\n",
    "    multiclass_classifier(X_CB_train_biobert,Y_CB_train_biobert,X_CB_test_biobert,Y_CB_test_biobert,model,str(model))\n",
    "    \n",
    "# LSTM Model\n",
    "X_CB_biobert=np.vstack(df_BA_APIRO_biobert['sent_embedding_array'])\n",
    "X_CB_biobert=X_CB_biobert[:, :300]\n",
    "#X_CB_biobert=abs(X_CB_biobert)\n",
    "Y_CB_biobert = df_BA_APIRO_biobert['aimoprc_category']\n",
    "Y_L_CB_biobert = pd.get_dummies(df_BA_APIRO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_L_CB_biobert.shape)\n",
    "X_CB_train_biobert, X_CB_test_biobert, Y_CB_train_biobert, Y_CB_test_biobert = train_test_split(X_CB_biobert,Y_CB_biobert, test_size = 0.10, random_state = 42)\n",
    "X_CB_train_LSTM_biobert, X_CB_test_LSTM_biobert,Y_CB_train_LSTM_biobert, Y_CB_test_LSTM_biobert = train_test_split(X_CB_biobert,Y_L_CB_biobert, test_size = 0.10, random_state = 42)\n",
    "print('train dataset shape')\n",
    "print(X_CB_train_biobert.shape,Y_CB_train_biobert.shape)\n",
    "print('test dataset shape')\n",
    "print(X_CB_test_biobert.shape,Y_CB_test_biobert.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_model_CB_biobert = Sequential()\n",
    "input_vector=300\n",
    "lstm_model_CB_biobert.add(LSTM(input_vector, dropout=0.4, recurrent_dropout=0.4))\n",
    "lstm_model_CB_biobert.add(Dense(5, activation='softmax'))\n",
    "lstm_model_CB_biobert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model_CB_biobert.build((None, input_vector,1))\n",
    "print(lstm_model_CB_biobert.summary())\n",
    "\n",
    "###\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = lstm_model_CB_biobert.fit(X_CB_train_LSTM_biobert, \n",
    "                                    Y_CB_train_LSTM_biobert, \n",
    "                                    epochs=epochs, \n",
    "                                    batch_size=batch_size,\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "                                   )\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "Y_CB_pred_biobert = np.argmax(lstm_model_CB_biobert.predict(X_CB_test_LSTM_biobert),axis=1)\n",
    "print(confusion_matrix(np.argmax(Y_CB_test_LSTM_biobert,axis=1),Y_CB_pred_biobert))\n",
    "print(classification_report(np.argmax(Y_CB_test_LSTM_biobert,axis=1),Y_CB_pred_biobert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid Dataset -Keras Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_covid_PICO= df_covid19_PICO_biobert['aimoprc_category']\n",
    "Y_L_covid_PICO = pd.get_dummies(df_covid19_PICO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_covid_PICO.shape)\n",
    "\n",
    "X_covid_PICO_train, X_covid_PICO_test, Y_covid_PICO_train, Y_covid_PICO_test = train_test_split(X_covid_PICO,Y_covid_PICO, test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_covid_PICO_train_LSTM, X_covid_PICO_test_LSTM,Y_covid_PICO_train_LSTM, Y_covid_PICO_test_LSTM = train_test_split(X_covid_PICO,Y_L_covid_PICO, test_size = 0.10, random_state = 42)\n",
    "\n",
    "print(X_covid_PICO_train.shape,Y_covid_PICO_train.shape)\n",
    "print(X_covid_PICO_test.shape,Y_covid_PICO_test.shape)\n",
    "\n",
    "# Base Model\n",
    "logreg_covid=LogisticRegression()\n",
    "knn_covid = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "adaboost_covid = AdaBoostClassifier(random_state=1)\n",
    "Gboost_covid = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "mlp_covid = MLPClassifier(hidden_layer_sizes=(100, 70, 30,20,10), max_iter=1000)\n",
    "\n",
    "run_models=[logreg_covid,knn_covid,adaboost_covid,Gboost_covid,mlp_covid]\n",
    "\n",
    "for model in run_models:\n",
    "    multiclass_classifier(X_covid_PICO_train,Y_covid_PICO_train,X_covid_PICO_test,Y_covid_PICO_test,model,str(model))\n",
    "    \n",
    "# LSTM Model\n",
    "lstm_model_covid = Sequential()\n",
    "lstm_model_covid.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_CB.shape[1]))\n",
    "lstm_model_covid.add(SpatialDropout1D(0.2))\n",
    "lstm_model_covid.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_covid.add(Dense(5, activation='softmax'))\n",
    "lstm_model_covid.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm_model_covid.summary())\n",
    "\n",
    "###\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = lstm_model_covid.fit(X_covid_PICO_train_LSTM, Y_covid_PICO_train_LSTM, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "Y_covid_PICO_pred = np.argmax(lstm_model_covid.predict(X_covid_PICO_test_LSTM),axis=1)\n",
    "print(confusion_matrix(np.argmax(Y_covid_PICO_test_LSTM,axis=1),Y_covid_PICO_pred))\n",
    "print(classification_report(np.argmax(Y_covid_PICO_test_LSTM,axis=1),Y_covid_PICO_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid Dataset-BioBERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_covid_PICO_biobert=np.vstack(df_covid19_PICO_biobert['sent_embedding_array'])\n",
    "X_covid_PICO_biobert=X_covid_PICO_biobert[:, :50]\n",
    "Y_covid_PICO_biobert= df_covid19_PICO_biobert['aimoprc_category']\n",
    "Y_L_covid_PICO_biobert = pd.get_dummies(df_covid19_PICO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_covid_PICO_biobert.shape)\n",
    "\n",
    "X_covid_PICO_train_biobert, X_covid_PICO_test_biobert, Y_covid_PICO_train_biobert, Y_covid_PICO_test_biobert = train_test_split(X_covid_PICO_biobert,Y_covid_PICO_biobert, test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_covid_PICO_train_LSTM_biobert, X_covid_PICO_test_LSTM_biobert,Y_covid_PICO_train_LSTM_biobert, Y_covid_PICO_test_LSTM_biobert = train_test_split(X_covid_PICO_biobert,Y_L_covid_PICO_biobert, test_size = 0.10, random_state = 42)\n",
    "\n",
    "print(X_covid_PICO_train_biobert.shape,Y_covid_PICO_train_biobert.shape)\n",
    "print(X_covid_PICO_test_biobert.shape,Y_covid_PICO_test_biobert.shape)\n",
    "\n",
    "# Base Model\n",
    "logreg_covid_biobert=LogisticRegression()\n",
    "knn_covid_biobert = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "adaboost_covid_biobert = AdaBoostClassifier(random_state=1)\n",
    "Gboost_covid_biobert = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "mlp_covid_biobert = MLPClassifier(hidden_layer_sizes=(100, 70, 30,20,10), max_iter=1000)\n",
    "\n",
    "run_models=[logreg_covid_biobert,knn_covid_biobert,adaboost_covid_biobert,Gboost_covid_biobert,mlp_covid_biobert]\n",
    "\n",
    "for model in run_models:\n",
    "    multiclass_classifier(X_covid_PICO_train_biobert,Y_covid_PICO_train_biobert,X_covid_PICO_test_biobert,Y_covid_PICO_test_biobert,model,str(model))\n",
    "# LSTM Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "X_covid_PICO_biobert=np.vstack(df_covid19_PICO_biobert['sent_embedding_array'])\n",
    "#X_covid_PICO_biobert=X_covid_PICO_biobert[:, :500]\n",
    "Y_covid_PICO_biobert= df_covid19_PICO_biobert['aimoprc_category']\n",
    "Y_L_covid_PICO_biobert = pd.get_dummies(df_covid19_PICO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_covid_PICO_biobert.shape)\n",
    "\n",
    "\n",
    "X_covid_PICO_train_LSTM_biobert, X_covid_PICO_test_LSTM_biobert,Y_covid_PICO_train_LSTM_biobert, Y_covid_PICO_test_LSTM_biobert = train_test_split(X_covid_PICO_biobert,Y_L_covid_PICO_biobert, test_size = 0.10, random_state = 42)\n",
    "print('train dataset shape')\n",
    "print(X_covid_PICO_train_LSTM_biobert.shape,Y_covid_PICO_train_LSTM_biobert.shape)\n",
    "print('test dataset shape')\n",
    "print(X_covid_PICO_test_LSTM_biobert.shape,Y_covid_PICO_test_LSTM_biobert.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_model_covid_biobert = Sequential()\n",
    "input_vector=768\n",
    "lstm_model_covid_biobert.add(LSTM(input_vector, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_covid_biobert.add(Dense(5, activation='softmax'))\n",
    "lstm_model_covid_biobert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model_covid_biobert.build((None, input_vector,1))\n",
    "print(lstm_model_covid_biobert.summary())\n",
    "\n",
    "###\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = lstm_model_covid_biobert.fit(X_covid_PICO_train_LSTM_biobert, \n",
    "                                    Y_covid_PICO_train_LSTM_biobert, \n",
    "                                    epochs=epochs, \n",
    "                                    batch_size=batch_size,\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "                                   )\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "Y_covid_PICO_pred_biobert = np.argmax(lstm_model_covid_biobert.predict(X_covid_PICO_test_LSTM_biobert),axis=1)\n",
    "print(confusion_matrix(np.argmax(Y_covid_PICO_test_LSTM_biobert,axis=1),Y_covid_PICO_pred_biobert))\n",
    "print(classification_report(np.argmax(Y_covid_PICO_test_LSTM_biobert,axis=1),Y_covid_PICO_pred_biobert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "X_covid_PICO_biobert=np.vstack(df_covid19_PICO_biobert['sent_embedding_array'])\n",
    "#X_covid_PICO_biobert=X_covid_PICO_biobert[:, :500]\n",
    "Y_covid_PICO_biobert= df_covid19_PICO_biobert['aimoprc_category']\n",
    "Y_L_covid_PICO_biobert = pd.get_dummies(df_covid19_PICO_biobert['aimoprc_category']).values\n",
    "print('Shape of label tensor:', Y_covid_PICO_biobert.shape)\n",
    "\n",
    "\n",
    "X_covid_PICO_train_LSTM_biobert, X_covid_PICO_test_LSTM_biobert,Y_covid_PICO_train_LSTM_biobert, Y_covid_PICO_test_LSTM_biobert = train_test_split(X_covid_PICO_biobert,Y_L_covid_PICO_biobert, test_size = 0.10, random_state = 42)\n",
    "print('train dataset shape')\n",
    "print(X_covid_PICO_train_LSTM_biobert.shape,Y_covid_PICO_train_LSTM_biobert.shape)\n",
    "print('test dataset shape')\n",
    "print(X_covid_PICO_test_LSTM_biobert.shape,Y_covid_PICO_test_LSTM_biobert.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_model_covid_biobert = Sequential()\n",
    "input_vector=768\n",
    "lstm_model_covid_biobert.add(LSTM(input_vector, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_covid_biobert.add(Dense(5, activation='softmax'))\n",
    "lstm_model_covid_biobert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model_covid_biobert.build((None, input_vector,1))\n",
    "print(lstm_model_covid_biobert.summary())\n",
    "\n",
    "###\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = lstm_model_covid_biobert.fit(X_covid_PICO_train_LSTM_biobert, \n",
    "                                    Y_covid_PICO_train_LSTM_biobert, \n",
    "                                    epochs=epochs, \n",
    "                                    batch_size=batch_size,\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "                                   )\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.title('CB Dataset Bi-LSTM bioBERT Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "Y_covid_PICO_pred_biobert = np.argmax(lstm_model_covid_biobert.predict(X_covid_PICO_test_LSTM_biobert),axis=1)\n",
    "print(confusion_matrix(np.argmax(Y_covid_PICO_test_LSTM_biobert,axis=1),Y_covid_PICO_pred_biobert))\n",
    "print(classification_report(np.argmax(Y_covid_PICO_test_LSTM_biobert,axis=1),Y_covid_PICO_pred_biobert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - Spark 2.3.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
