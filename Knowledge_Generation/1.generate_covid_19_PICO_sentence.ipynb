{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn\n",
    "!pip install nltk\n",
    "!pip install keras\n",
    "!pip install seaborn\n",
    "!pip install tensorflow\n",
    "!pip install biobert-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 18:22:45.298825: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-29 18:22:46.133981: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-09-29 18:22:46.134016: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-29 18:22:46.222441: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-29 18:22:48.052885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-09-29 18:22:48.053122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oss/hadoop/lib/native/\n",
      "2022-09-29 18:22:48.053141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "#from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#tqdm.pandas(desc=\"progress-bar\")\n",
    "#from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import gensim\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,make_scorer,precision_score,recall_score,roc_auc_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import xgboost as xgb\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "#from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "#import plotly.graph_objs as go\n",
    "#import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "#from plotly.offline import iplot\n",
    "#cufflinks.go_offline()\n",
    "#cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "from bs4 import BeautifulSoup\n",
    "#from rouge import Rouge\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import getpass\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation \n",
    "\n",
    "from biobert_embedding.embedding import BiobertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read metadata .csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/oss/conda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (5,13,14,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_covid19_raw=pd.read_csv('./data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139061 entries, 0 to 139060\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   cord_uid           139061 non-null  object\n",
      " 1   abstract           139061 non-null  object\n",
      " 2   Abstract_Cat_List  139061 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_covid19_raw.info(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append end: at the end of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def appendAbstract(cols):\n",
    "    '''\n",
    "    @Author - Fakhare Alam\n",
    "    '''\n",
    "    abstract = cols[0]\n",
    "    if pd.isnull(abstract):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return abstract+'end:'\n",
    "    \n",
    "df_covid19_raw['abstract'] = df_covid19_raw[['abstract']].apply(appendAbstract,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define master list for different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_cat_list=['objective:', 'background:', 'background and objectives:', 'context:', 'background and purpose:',\n",
    "                 'purpose:', 'importance:', 'introduction:', 'aim:', 'rationale:', 'goal:', 'context:', 'hypothesis:'\n",
    "                ,'population:', 'participant:', 'sample:', 'subject:', 'patient:','patient 2:','patient 1:',\n",
    "                 'intervention:', 'diagnosis:','prognosis:'\n",
    "                ,'outcome:', 'measure:', 'variable:', 'assessment:'\n",
    "                ,'method:','methods:' ,'setting:', 'design:', 'material:', 'procedure:', 'process:', 'methodology:'\n",
    "                ,'result:','results:','finding:',\n",
    "                 'conclusion:','conclusions:', 'implication:', 'discussion:', 'interpretation table:','end:']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping to find keyword in each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractCatList(cols):\n",
    "    abstract=cols['abstract']\n",
    "    abstract_cat_list={}\n",
    "    for cat in master_cat_list:\n",
    "        indexpos=abstract.lower().find(cat.lower())\n",
    "        if (indexpos!=-1):\n",
    "            abstract_cat_list[indexpos]=cat\n",
    "    return abstract_cat_list\n",
    "\n",
    "df_covid19_raw['Abstract_Cat_List'] = df_covid19_raw[['abstract']].apply(abstractCatList,axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_raw=df_covid19_raw[['cord_uid','abstract','Abstract_Cat_List']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new dataframe with filled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_raw_split=pd.DataFrame(columns=df_covid19_raw.columns)\n",
    "\n",
    "for index, row in df_covid19_raw.iterrows():\n",
    "    #print(row('Abstract'))\n",
    "    if(index % 1000==0):\n",
    "        print('completed',index)\n",
    "    od = collections.OrderedDict(sorted(row['Abstract_Cat_List'].items()))\n",
    "    key_list=list(od.keys())\n",
    "    s=row['abstract'].lower()\n",
    "    for i in range(0,len(key_list)-1,1):\n",
    "        try:\n",
    "            start=row['Abstract_Cat_List'][key_list[i]]\n",
    "            end=row['Abstract_Cat_List'][key_list[i+1]]\n",
    "            result = re.search('%s(.*)%s' % (start, end), s).group(1)\n",
    "            row[start]=result\n",
    "        except:\n",
    "            continue\n",
    "    df_covid19_raw_split=df_covid19_raw_split.append(row,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_raw_split.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=['objective:', 'background:', 'background and objectives:', 'context:','purpose:', 'importance:', 'introduction:', 'aim:', 'rationale:', 'goal:', 'context:', 'hypothesis:']\n",
    "df_covid19_raw_split['|a|']=df_covid19_raw_split[A].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=['population:', 'participant:', 'sample:', 'subject:', 'patient:']\n",
    "df_covid19_raw_split['|p|']=df_covid19_raw_split[P].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=['diagnosis:','intervention:']\n",
    "df_covid19_raw_split['|i|']=df_covid19_raw_split[I].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "O=['outcome:', 'measure:', 'variable:', 'assessment:']\n",
    "df_covid19_raw_split['|o|']=df_covid19_raw_split[O].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=['method:','methods:' ,'setting:', 'design:', 'material:', 'procedure:', 'process:', 'methodology:']\n",
    "df_covid19_raw_split['|m|']=df_covid19_raw_split[M].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=['result:','results:','finding:']\n",
    "df_covid19_raw_split['|r|']=df_covid19_raw_split[R].apply(lambda x: ''.join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=['conclusion:','conclusions:', 'implication:', 'discussion:']\n",
    "df_covid19_raw_split['|c|']=df_covid19_raw_split[C].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "aimoprc_list=['|a|', '|i|', '|m|', '|o|','|p|', '|r|', '|c|']\n",
    "# Test\n",
    "df_covid19_PICO=pd.DataFrame(columns=['sent','aimoprc_category'])\n",
    "for aimoprc in aimoprc_list:\n",
    "    df_temp_covid19_PICO=pd.DataFrame(columns=['sent','aimoprc_category'])\n",
    "    df_temp_covid19_PICO['sent']=df_covid19_raw_split[aimoprc]\n",
    "    df_temp_covid19_PICO['aimoprc_category']=aimoprc\n",
    "    df_covid19_PICO=df_covid19_PICO.append(df_temp_covid19_PICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid19_PICO.replace('', np.nan, inplace=True)\n",
    "df_covid19_PICO.dropna(inplace=True)\n",
    "df_covid19_PICO['aimoprc_category']=df_covid19_PICO['aimoprc_category'].map({'|a|':'A', '|i|':'I', '|m|':'M', '|o|':'O','|p|':'P', '|r|':'R', '|c|':'C'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesCO = {'C':'O'}\n",
    "# Merge {M,P},{C,O}\n",
    "df_covid19_PICO['aimoprc_category'].replace(rulesCO,inplace=True)\n",
    "rulesMP = {'M':'P'}\n",
    "df_covid19_PICO['aimoprc_category'].replace(rulesMP,inplace=True)\n",
    "\n",
    "# Drop NULL Sentences\n",
    "df_covid19_PICO.dropna(axis=0,how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    10331\n",
       "A    10152\n",
       "P    10127\n",
       "R     9683\n",
       "I       52\n",
       "Name: aimoprc_category, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid19_PICO['aimoprc_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_covid19_PICO.to_csv('./data/df_covid19_PICO.csv',index=False)\n",
    "#A->P->I->R-O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - Spark 2.3.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
